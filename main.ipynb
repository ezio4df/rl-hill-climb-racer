{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Android gameplay footage (Hill Climb Racing) via scrcpy ‚Üí virtual cam ‚Üí Python\n",
    "\n"
   ],
   "id": "8c65e303d6aea1e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "------",
   "id": "169cb4ab7f87b653"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Attempt 01",
   "id": "4e0bcbf350da1b26"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "--------",
   "id": "6319ff7746494998"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Running the model",
   "id": "ab63e34b59277a6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import subprocess\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from IPython.core.display_functions import clear_output\n",
    "from PIL import Image\n"
   ],
   "id": "4da49986ed2eb2ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === CONFIG ===\n",
    "ACCEL_POS = (200, 850, 240, 850, 500)  #(x1, y1, x2, y2)\n",
    "BRAKE_POS = (2115, 850, 2200, 850, 500)  #(x1, y1, x2, y2)\n",
    "VIDEO_DEVICE = '/dev/video10'\n",
    "HOLD_DURATION_MS = 10_000  # 10 seconds (long enough to cover many steps)\n",
    "TAP_DURATION_MS = 1  # 10 seconds (long enough to cover many steps)"
   ],
   "id": "50df4e086bd9f72c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "....",
   "id": "86c5e4c186a76c86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === HELPERS ===\n",
    "def start_hold(x, y, duration_ms=HOLD_DURATION_MS):\n",
    "    \"\"\"Start a long press in background (non-blocking)\"\"\"\n",
    "    return subprocess.Popen([\n",
    "        'adb', 'shell', 'input', 'swipe',\n",
    "        str(x), str(y), str(x), str(y), str(duration_ms)\n",
    "    ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "\n",
    "def stop_hold(proc):\n",
    "    \"\"\"Gracefully stop a running hold\"\"\"\n",
    "    if proc is not None:\n",
    "        proc.terminate()\n",
    "        try:\n",
    "            proc.wait(timeout=0.1)\n",
    "        except subprocess.TimeoutExpired:\n",
    "            proc.kill()\n",
    "\n",
    "\n",
    "def get_action_from_model(frame):\n",
    "    \"\"\"\n",
    "    Replace this with your PyTorch model.\n",
    "    Must return one of: \"accel\", \"brake\", \"none\"\n",
    "    \"\"\"\n",
    "    # Placeholder: random action (replace with real inference)\n",
    "    time.sleep(0.02)  # Simulate model latency (~20ms)\n",
    "    return np.random.choice([\"accel\", \"brake\", \"none\"])\n",
    "\n",
    "\n",
    "def get_frame(cap):\n",
    "    \"\"\"Grab single frame from video device\"\"\"\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        raise RuntimeError(\"Failed to read frame\")\n",
    "    return frame\n",
    "\n",
    "\n",
    "def resize_img(frame, width=84):\n",
    "    \"\"\"Resize frame to given width, preserving aspect ratio (height auto-scaled)\"\"\"\n",
    "    h, w = frame.shape[:2]\n",
    "    new_w = width\n",
    "    new_h = int(h * (new_w / w))\n",
    "    resized = cv2.resize(frame, (new_w, new_h))\n",
    "    return resized\n",
    "\n",
    "\n",
    "def ocr_distance(frame):\n",
    "    \"\"\"Extract distance from frame via OCR. Return float distance in meters.\"\"\"\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    # ADJUST THESE CROP COORDINATES TO YOUR SCREEN\n",
    "    x1, x2 = 1800, 2200\n",
    "    y1, y2 = 20, 60\n",
    "\n",
    "    # Fix bounds\n",
    "    x1 = max(0, min(x1, w))\n",
    "    x2 = max(0, min(x2, w))\n",
    "    y1 = max(0, min(y1, h))\n",
    "    y2 = max(0, min(y2, h))\n",
    "\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        print(\"OCR: Invalid crop region\")\n",
    "        return 0.0\n",
    "\n",
    "    roi = frame[y1:y2, x1:x2]\n",
    "\n",
    "    # Only show if valid\n",
    "    if roi.size > 0:\n",
    "        cv2.imshow(\"OCR Region\", roi)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "    # TODO: Add OCR here later\n",
    "    return 0.0"
   ],
   "id": "33ec36b99bd0577f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## stream the screen though adb",
   "id": "2b11fd8adae06de9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# Start scrcpy in background\n",
    "scrcpy_proc = subprocess.Popen([\n",
    "    'scrcpy', f'--v4l2-sink={VIDEO_DEVICE}',\n",
    "    '--no-window', '--max-size', '720', '--stay-awake'\n",
    "], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_DEVICE)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(f\"Cannot open {VIDEO_DEVICE}\")"
   ],
   "id": "17baba39fa20da39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## OCR distance",
   "id": "1c18da7012842a35"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test: ocr distance from screen using `easyocr`",
   "id": "f09e2b061613c742"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "import easyocr\n",
    "\n",
    "reader = easyocr.Reader(['en'], gpu=True)\n",
    "\n",
    "\n",
    "def ocr_distance(frame):\n",
    "    \"\"\"Extract distance from frame via OCR. Return float distance in meters.\"\"\"\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    # ADJUST THESE CROP COORDINATES TO YOUR SCREEN\n",
    "    x1, y1 = 240, 24\n",
    "    x2, y2 = x1 + 70, y1 + 19\n",
    "\n",
    "    # Fix bounds\n",
    "    x1 = max(0, min(x1, w))\n",
    "    x2 = max(0, min(x2, w))\n",
    "    y1 = max(0, min(y1, h))\n",
    "    y2 = max(0, min(y2, h))\n",
    "\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return 0\n",
    "\n",
    "    roi = frame[y1:y2, x1:x2]\n",
    "    if roi.size == 0:\n",
    "        return 0\n",
    "\n",
    "    # Preprocessing for better OCR:\n",
    "    # Preprocessing for better OCR:\n",
    "    # Preprocessing for better OCR:\n",
    "    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Display (for debugging in notebook)\n",
    "    display(Image.fromarray(thresh, 'L'))\n",
    "\n",
    "    # Run EasyOCR on preprocessed image\n",
    "    try:\n",
    "        result = reader.readtext(thresh, detail=0, allowlist='0123456789m')\n",
    "        if result:\n",
    "            text = result[0].strip().lower().replace('m', '')\n",
    "            if text.isdigit():\n",
    "                print(\"OCR Distance: \", text)\n",
    "                return int(text)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    return 0\n",
    "\n",
    "# while True:\n",
    "#     clear_output()\n",
    "#\n",
    "#     # # ret, frame = cap.read()\n",
    "#     frame = get_frame(cap)\n",
    "#     # rframe = resize_img(frame)\n",
    "#     # Image.fromarray(frame)\n",
    "#     # Image.fromarray(rframe)\n",
    "#     ocr_distance(frame)\n",
    "#     # len(frame)\n",
    "#     time.sleep(50 / 1000)\n"
   ],
   "id": "d008334870228464",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test: ocr using PaddleOCR",
   "id": "f54dd8c6ceab198e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from paddleocr import PaddleOCR\n",
    "\n",
    "ocr_engine = PaddleOCR(use_angle_cls=False, lang='en')"
   ],
   "id": "cd4496650a7c8d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ADD ONCE AT TOP (after imports)\n",
    "from paddleocr import PaddleOCR\n",
    "\n",
    "ocr_engine = PaddleOCR(use_angle_cls=False, lang='en')\n",
    "\n",
    "\n",
    "def ocr_distance(frame):\n",
    "    h, w = frame.shape[:2]\n",
    "    x1, y1 = 240, 24\n",
    "    x2, y2 = x1 + 70, y1 + 19\n",
    "\n",
    "    x1 = max(0, min(x1, w))\n",
    "    x2 = max(0, min(x2, w))\n",
    "    y1 = max(0, min(y1, h))\n",
    "    y2 = max(0, min(y2, h))\n",
    "\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return 0\n",
    "\n",
    "    roi = frame[y1:y2, x1:x2]\n",
    "    if roi.size == 0:\n",
    "        return 0\n",
    "\n",
    "    # Correct call: .ocr(), NOT callable\n",
    "    result = ocr_engine.ocr(roi, cls=False)\n",
    "    if result and len(result[0]) > 0:\n",
    "        text = result[0][0][1][0]\n",
    "        num_str = ''.join(filter(str.isdigit, text))\n",
    "        if num_str:\n",
    "            return int(num_str)\n",
    "    return 0\n",
    "\n",
    "# while True:\n",
    "#     clear_output()\n",
    "#\n",
    "#     # # ret, frame = cap.read()\n",
    "#     frame = get_frame(cap)\n",
    "#     # rframe = resize_img(frame)\n",
    "#     # Image.fromarray(frame)\n",
    "#     # Image.fromarray(rframe)\n",
    "#     ocr_distance(frame)\n",
    "#     # len(frame)\n",
    "#     time.sleep(500 / 1000)"
   ],
   "id": "b877df3a0607848a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test: ocr using Tesseract-OCR",
   "id": "fc989cb5d3f96751"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ADD ONCE AT TOP (after imports)\n",
    "import pytesseract"
   ],
   "id": "a86d7914ed8d5c78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def ocr_distance(frame):\n",
    "    h, w = frame.shape[:2]\n",
    "    x1, y1 = 240, 24\n",
    "    x2, y2 = x1 + 70, y1 + 19\n",
    "\n",
    "    x1 = max(0, min(x1, w))\n",
    "    x2 = max(0, min(x2, w))\n",
    "    y1 = max(0, min(y1, h))\n",
    "    y2 = max(0, min(y2, h))\n",
    "\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return 0\n",
    "\n",
    "    roi = frame[y1:y2, x1:x2]\n",
    "    if roi.size == 0:\n",
    "        return 0\n",
    "\n",
    "    # Preprocess for Tesseract: B&W, high contrast\n",
    "    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY)\n",
    "    thresh = cv2.bitwise_not(thresh)\n",
    "    display(Image.fromarray(thresh))\n",
    "\n",
    "    # Run Tesseract: only digits and 'm'\n",
    "    text = pytesseract.image_to_string(\n",
    "        thresh,\n",
    "        config='--psm 8 --oem 0 -c tessedit_char_whitelist=0123456789m'\n",
    "    )\n",
    "    num_str = ''.join(filter(str.isdigit, text))\n",
    "    return int(num_str) if num_str else 0\n",
    "\n",
    "\n",
    "while True:\n",
    "    clear_output()\n",
    "\n",
    "    # # ret, frame = cap.read()\n",
    "    frame = get_frame(cap)\n",
    "    # rframe = resize_img(frame)\n",
    "    # Image.fromarray(frame)\n",
    "    # Image.fromarray(rframe)\n",
    "    print(ocr_distance(frame))\n",
    "    # len(frame)\n",
    "    time.sleep(500 / 1000)\n",
    "    break"
   ],
   "id": "556216ab3ec38e20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test: ocr using template",
   "id": "c89ebb3014d03a84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load 0-9 character templates, for ocr'ing the distance from screen\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "def load_chr_templates():\n",
    "    templates = {}\n",
    "    for i in range(10):\n",
    "        path = f\"assets/chr_{i}.png\"\n",
    "        if os.path.exists(path):\n",
    "            img = cv2.imread(path, cv2.IMREAD_UNCHANGED)  # loads BGRA\n",
    "            templates[str(i)] = img\n",
    "    return templates\n",
    "\n",
    "\n",
    "chr_templates = load_chr_templates()"
   ],
   "id": "b938473e06ad9dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "def ocr_distance(frame):\n",
    "    h, w = frame.shape[:2]\n",
    "    x1, y1 = 240, 24\n",
    "    x2, y2 = x1 + 70, y1 + 19\n",
    "\n",
    "    x1 = max(0, min(x1, w))\n",
    "    x2 = max(0, min(x2, w))\n",
    "    y1 = max(0, min(y1, h))\n",
    "    y2 = max(0, min(y2, h))\n",
    "\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return 0\n",
    "\n",
    "    roi = frame[y1:y2, x1:x2]\n",
    "    if roi.size == 0:\n",
    "        return 0\n",
    "\n",
    "    # Convert ROI to grayscale (once)\n",
    "    # roi_gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY).astype(np.float32)\n",
    "    roi_gray = roi\n",
    "\n",
    "    display(Image.fromarray(roi_gray))\n",
    "\n",
    "    text = \"\"\n",
    "    pos = 0\n",
    "    while pos < roi_gray.shape[1]:\n",
    "        best_char = None\n",
    "        min_diff = float('inf')\n",
    "        for char, tmpl in chr_templates.items():\n",
    "            if tmpl is None:\n",
    "                continue\n",
    "            h_t, w_t = tmpl.shape[:2]\n",
    "            if pos + w_t > roi_gray.shape[1] or h_t != roi_gray.shape[0]:\n",
    "                continue\n",
    "\n",
    "            # Extract patch (no resize)\n",
    "            patch = roi_gray[:, pos:pos + w_t]\n",
    "            if patch.shape != (h_t, w_t):\n",
    "                continue\n",
    "\n",
    "            # Extract alpha (opacity) ‚Äî ignore if < 1/255\n",
    "            alpha = tmpl[:, :, 3].astype(np.float32) / 255.0\n",
    "            mask = alpha > 0.01  # treat near-transparent as transparent\n",
    "\n",
    "            if not np.any(mask):\n",
    "                continue\n",
    "\n",
    "            # Template grayscale\n",
    "            tmpl_gray = cv2.cvtColor(tmpl[:, :, :3], cv2.COLOR_BGR2GRAY).astype(np.float32)\n",
    "\n",
    "            # Compute absolute difference only on opaque pixels\n",
    "            diff = np.abs(patch - tmpl_gray) * mask\n",
    "            total_diff = np.sum(diff) / np.sum(mask)  # normalize by valid pixels\n",
    "\n",
    "            if total_diff < min_diff and total_diff < 30.0:  # threshold tuned\n",
    "                min_diff = total_diff\n",
    "                best_char = char\n",
    "\n",
    "        if best_char:\n",
    "            text += best_char\n",
    "            pos += chr_templates[best_char].shape[1]  # advance by digit width\n",
    "        else:\n",
    "            pos += 1  # move 1px if no match\n",
    "\n",
    "    print(text)\n",
    "    return int(text) if text.isdigit() else 0\n",
    "\n",
    "\n",
    "frame = get_frame(cap)\n",
    "ocr_distance(frame)"
   ],
   "id": "5c271081fc3017c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train custom model for accurate ocr of the distance!",
   "id": "26722e7bd8e9b779"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import hashlib\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def save_unique_image(img: Image.Image, folder=\"assets/extracts\"):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    # Compute hash of the image\n",
    "    img_bytes = img.tobytes()\n",
    "    img_hash = hashlib.md5(img_bytes).hexdigest()\n",
    "    filepath = os.path.join(folder, f\"{img_hash}.png\")\n",
    "\n",
    "    # Save only if not exists\n",
    "    if not os.path.exists(filepath):\n",
    "        img.save(filepath)\n",
    "        return True\n",
    "    return False"
   ],
   "id": "897e448e51d25550",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_distance_img(frame):\n",
    "    h, w = frame.shape[:2]\n",
    "    x1, y1 = 240, 24\n",
    "    x2, y2 = x1 + 70, y1 + 19\n",
    "\n",
    "    x1 = max(0, min(x1, w))\n",
    "    x2 = max(0, min(x2, w))\n",
    "    y1 = max(0, min(y1, h))\n",
    "    y2 = max(0, min(y2, h))\n",
    "\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return 0\n",
    "\n",
    "    roi = frame[y1:y2, x1:x2]\n",
    "    if roi.size == 0:\n",
    "        return 0\n",
    "\n",
    "    # Preprocess for Tesseract: B&W, high contrast\n",
    "    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY)\n",
    "    thresh = cv2.bitwise_not(thresh)\n",
    "\n",
    "    # display(Image.fromarray(thresh))\n",
    "    return thresh"
   ],
   "id": "10f02f3ae358ea20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "frame = get_frame(cap)\n",
    "\n",
    "# while True:\n",
    "clear_output()\n",
    "start_iter = time.perf_counter()\n",
    "\n",
    "distance_img = Image.fromarray(get_distance_img(frame))\n",
    "did_save = save_unique_image(distance_img)\n",
    "\n",
    "display(distance_img)\n",
    "iter_time = time.perf_counter() - start_iter\n",
    "print(f\"\\r took: {iter_time * 1000:.1f} ms | {'saved!' if did_save else ''}\", end=\"\")\n",
    "\n"
   ],
   "id": "e9c671278c39bbb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def split_digits(roi):\n",
    "    \"\"\"\n",
    "    Split a distance ROI into individual digit images.\n",
    "    Handles thin digits (like '1') by using morphological closing.\n",
    "    \"\"\"\n",
    "    if len(roi.shape) == 3:\n",
    "        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = roi.copy()\n",
    "\n",
    "    # Ensure white digits on black background\n",
    "    _, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # Connect nearby pixels (helps thin '1')\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    digit_bboxes = []\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        if w >= 2 and h >= 8:  # allow very thin but tall (like '1')\n",
    "            digit_bboxes.append((x, y, w, h))\n",
    "\n",
    "    digit_bboxes = sorted(digit_bboxes, key=lambda b: b[0])\n",
    "    digits = []\n",
    "    for (x, y, w, h) in digit_bboxes:\n",
    "        digit = roi[y:y + h, x:x + w]\n",
    "        digits.append(digit)\n",
    "\n",
    "    return digits\n",
    "\n",
    "\n",
    "def scale_pil_image(img: Image.Image, scale: int) -> Image.Image:\n",
    "    \"\"\"Scale up a PIL image by integer factor using nearest neighbor.\"\"\"\n",
    "    return img.resize((img.width * scale, img.height * scale), Image.NEAREST)"
   ],
   "id": "c84e7508e790aaa7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Get a random labeled image\n",
    "labeled_dir = \"assets/labeled\"\n",
    "labeled_images = [f for f in os.listdir(labeled_dir) if f.endswith('.png')]\n",
    "if not labeled_images:\n",
    "    raise FileNotFoundError(\"No labeled images found!\")\n",
    "\n",
    "random_img = random.choice(labeled_images)\n",
    "img_path = os.path.join(labeled_dir, random_img)\n",
    "print(f\"Random image: {random_img}\")\n",
    "\n",
    "# Load image\n",
    "roi = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "display(scale_pil_image(Image.fromarray(roi), scale=10))\n",
    "\n",
    "# Split digits\n",
    "digit_images = split_digits(roi)\n",
    "\n",
    "# Display each digit in notebook\n",
    "print(\"Split digits:\")\n",
    "for i, digit in enumerate(digit_images):\n",
    "    # Convert OpenCV (BGR/gray) to PIL for notebook display\n",
    "    pil_img = Image.fromarray(digit)\n",
    "    pil_img = scale_pil_image(pil_img, scale=10)\n",
    "    display(pil_img)\n",
    "    print(f\"Digit {i}: shape {digit.shape}\")"
   ],
   "id": "d3aed05e100e1b35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### train the moedl on the dataset",
   "id": "fafbee303acc1b56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import random\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "315248c4fe00bc12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def char_to_class(c):\n",
    "    if c == 'm':\n",
    "        return 10\n",
    "    return int(c)\n",
    "\n",
    "\n",
    "def class_to_char(cls):\n",
    "    if cls == 10:\n",
    "        return 'm'\n",
    "    return str(cls)"
   ],
   "id": "58cb2fe5f18fe9de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 1. DATASET ---\n",
    "class DigitM_Dataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        for filename in os.listdir(root_dir):\n",
    "            if not filename.endswith('.png'):\n",
    "                continue\n",
    "            # Parse full label INCLUDING 'm'\n",
    "            label_str = filename.split('-')[0]  # \"123m-05.png\" ‚Üí \"123m\"\n",
    "\n",
    "            img_path = os.path.join(root_dir, filename)\n",
    "            roi = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if roi is None:\n",
    "                continue\n",
    "\n",
    "            digits = split_digits(roi)\n",
    "            if len(digits) == len(label_str):\n",
    "                for i, digit_img in enumerate(digits):\n",
    "                    char = label_str[i]\n",
    "\n",
    "                    if char == 'm':\n",
    "                        digit_label = 10\n",
    "                    else:\n",
    "                        digit_label = int(char)\n",
    "\n",
    "                    self.samples.append((digit_img, digit_label))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.samples[idx]\n",
    "        img_resized = cv2.resize(img, (20, 20), interpolation=cv2.INTER_AREA)\n",
    "        img_pil = Image.fromarray(img_resized)\n",
    "        if self.transform:\n",
    "            img_pil = self.transform(img_pil)\n",
    "        return img_pil, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "\n",
    "# --- 2. MODEL ---\n",
    "class DigitClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=11):  # CHANGED: 11 classes (0-9 + 'm')\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((4, 4))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 4 * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)  # Now 11 outputs\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ],
   "id": "7d04fe69a2b08b30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 3. TRAINING SETUP ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = DigitM_Dataset(\"assets/labeled\", transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model = DigitClassifier(num_classes=11).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "d506d18f70e27bb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 4. TRAIN LOOP ---\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Test\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {train_loss / len(train_loader):.4f}, \"\n",
    "          f\"Test Acc: {100 * correct / total:.2f}%\")"
   ],
   "id": "66f2e2db35087948",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "524c6118645b28a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Assume split_digits() is already defined (from earlier)\n",
    "def split_digits(roi):\n",
    "    if len(roi.shape) == 3:\n",
    "        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = roi.copy()\n",
    "    _, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    digit_bboxes = []\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        if w >= 2 and h >= 8:\n",
    "            digit_bboxes.append((x, y, w, h))\n",
    "    digit_bboxes = sorted(digit_bboxes, key=lambda b: b[0])\n",
    "    digits = []\n",
    "    for (x, y, w, h) in digit_bboxes:\n",
    "        digit = roi[y:y + h, x:x + w]\n",
    "        digits.append(digit)\n",
    "    return digits\n",
    "\n",
    "\n",
    "# --- DATASET PROFILING ---\n",
    "labeled_dir = \"assets/labeled\"\n",
    "valid_samples = 0\n",
    "total_images = 0\n",
    "mismatches = []\n",
    "\n",
    "print(\"üîç Profiling dataset...\\n\")\n",
    "\n",
    "for filename in sorted(os.listdir(labeled_dir)):\n",
    "    if not filename.endswith('.png'):\n",
    "        continue\n",
    "\n",
    "    total_images += 1\n",
    "    # Parse label: e.g., \"123m-05.png\" ‚Üí \"123\"\n",
    "    try:\n",
    "        label_str = filename.split('-')[0]  # \"123m\"\n",
    "        if not label_str[:-1].isdigit() and label_str[-1] == 'm':\n",
    "            continue\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    img_path = os.path.join(labeled_dir, filename)\n",
    "    roi = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if roi is None:\n",
    "        continue\n",
    "\n",
    "    digits = split_digits(roi)\n",
    "    label_len = len(label_str)\n",
    "    digit_len = len(digits)\n",
    "\n",
    "    if digit_len == label_len:\n",
    "        valid_samples += 1\n",
    "        # Show first 3 valid samples\n",
    "        if valid_samples <= 3:\n",
    "            print(f\"‚úÖ VALID: {filename} | Label: '{label_str}' | Digits found: {digit_len}\")\n",
    "            # Display original\n",
    "            plt.figure(figsize=(8, 2))\n",
    "            plt.subplot(1, len(digits) + 1, 1)\n",
    "            plt.imshow(roi, cmap='gray')\n",
    "            plt.title(\"Original ROI\")\n",
    "            plt.axis('off')\n",
    "            # Display each digit\n",
    "            for i, d in enumerate(digits):\n",
    "                char = label_str[i]\n",
    "                class_id = char_to_class(char)\n",
    "                plt.subplot(1, len(digits) + 1, i + 2)\n",
    "                plt.imshow(d, cmap='gray')\n",
    "                plt.title(f\"Char '{char}' (cls {class_id})\")\n",
    "                plt.axis('off')\n",
    "            plt.show()\n",
    "            print(\"-\" * 50)\n",
    "    else:\n",
    "        mismatches.append((filename, label_str, digit_len))\n",
    "\n",
    "print(f\"\\nüìä REPORT:\")\n",
    "print(f\"Total images: {total_images}\")\n",
    "print(f\"Valid samples: {valid_samples}\")\n",
    "print(f\"Mismatches: {len(mismatches)}\")\n",
    "\n",
    "if mismatches:\n",
    "    print(\"\\n‚ö†Ô∏è MISMATCH EXAMPLES:\")\n",
    "    for fname, label, dlen in mismatches[:5]:\n",
    "        print(f\"  {fname} ‚Üí label='{label}' (len={len(label)}), digits={dlen}\")\n",
    "\n",
    "\"\"\"\n",
    "10000m-00.png\n",
    "1025m-01.png\n",
    "1042m-00.png\n",
    "\"\"\""
   ],
   "id": "ad77137c6db36975",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 5. SAVE MODEL ---\n",
    "torch.save(model.state_dict(), \"digit_model.pth\")\n",
    "print(\"Model saved as digit_model.pth\")"
   ],
   "id": "9dea5aaa7bf69ea5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# run the model",
   "id": "aeacc40f7f44ce78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "current_action = None\n",
    "current_hold_proc = None\n",
    "\n",
    "timings = []\n",
    "step = 0\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        start_iter = time.perf_counter()\n",
    "\n",
    "        # Get frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        # Run model\n",
    "        new_action = get_action_from_model(frame)\n",
    "\n",
    "        # Execute action if changed\n",
    "        if new_action != current_action:\n",
    "            if current_hold_proc is not None:\n",
    "                current_hold_proc.terminate()\n",
    "                current_hold_proc = None\n",
    "\n",
    "            if new_action == \"accel\":\n",
    "                # 1ms tap\n",
    "                subprocess.run([\n",
    "                    'adb', 'shell', 'input', 'swipe',\n",
    "                    str(ACCEL_POS[0]), str(ACCEL_POS[1]),\n",
    "                    str(ACCEL_POS[2]), str(ACCEL_POS[3]),\n",
    "                    str(TAP_DURATION_MS)\n",
    "                ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "                # Long hold\n",
    "                current_hold_proc = subprocess.Popen([\n",
    "                    'adb', 'shell', 'input', 'swipe',\n",
    "                    str(ACCEL_POS[0]), str(ACCEL_POS[1]),\n",
    "                    str(ACCEL_POS[2]), str(ACCEL_POS[3]),\n",
    "                    str(HOLD_DURATION_MS)\n",
    "                ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "            elif new_action == \"brake\":\n",
    "                # 1ms tap\n",
    "                subprocess.run([\n",
    "                    'adb', 'shell', 'input', 'swipe',\n",
    "                    str(BRAKE_POS[0]), str(BRAKE_POS[1]),\n",
    "                    str(BRAKE_POS[2]), str(BRAKE_POS[3]),\n",
    "                    str(TAP_DURATION_MS)\n",
    "                ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "                # Long hold\n",
    "                current_hold_proc = subprocess.Popen([\n",
    "                    'adb', 'shell', 'input', 'swipe',\n",
    "                    str(BRAKE_POS[0]), str(BRAKE_POS[1]),\n",
    "                    str(BRAKE_POS[2]), str(BRAKE_POS[3]),\n",
    "                    str(HOLD_DURATION_MS)\n",
    "                ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "            current_action = new_action\n",
    "\n",
    "        # Timing\n",
    "        iter_time = time.perf_counter() - start_iter\n",
    "        timings.append(iter_time)\n",
    "        if len(timings) > 100:\n",
    "            timings = timings[-100:]\n",
    "        avg_time = sum(timings) / len(timings)\n",
    "        fps = 1.0 / avg_time if avg_time > 0 else 0\n",
    "\n",
    "        print(f\"took: {iter_time * 1000:.1f} ms | avg took: {avg_time * 1000:.1f} ms | fps: {fps:.1f}\")\n",
    "\n",
    "        step += 1\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    if current_hold_proc is not None:\n",
    "        current_hold_proc.terminate()\n",
    "    cap.release()"
   ],
   "id": "a735f1080f849ec0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Making the model\n",
   "id": "3d767acd44343916"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# === LOAD YOUR MODEL HERE ===\n",
    "class HillClimbRacerV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 19 * 7, 128),  # input size matches 34x83 ‚Üí (34-5)//2+1=15 ‚Üí wait, recalc properly\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda')\n",
    "model = HillClimbRacerV1().to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# If you have a saved model, load it:\n",
    "# model.load_state_dict(torch.load('your_model.pth', map_location=device))\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"Convert raw frame to model input tensor (1, 1, H, W)\"\"\"\n",
    "    # Example: crop, resize, grayscale, normalize\n",
    "    # ADJUST CROP/RESIZE TO YOUR GAME UI\n",
    "    cropped = frame[100:900, 200:2200]  # adjust based on your screen\n",
    "    resized = cv2.resize(cropped, (84, 84))\n",
    "    gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
    "    tensor = torch.from_numpy(gray).float() / 255.0\n",
    "    tensor = tensor.unsqueeze(0).unsqueeze(0)  # (1, 1, 84, 84)\n",
    "    return tensor.to(device)\n",
    "\n",
    "\n",
    "def get_action_from_model(frame):\n",
    "    tensor = preprocess_frame(frame)\n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor)\n",
    "        pred = torch.argmax(logits, dim=1).item()\n",
    "    actions = [\"accel\", \"brake\", \"none\"]\n",
    "    return actions[pred]"
   ],
   "id": "dbcd9c329b4a120f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "------",
   "id": "a9085db8b5bdad01"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Attempt 01",
   "id": "9abeace803511141"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "--------",
   "id": "a78a87777576f9f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# init",
   "id": "ade2980f4dde0afb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T12:52:58.776949Z",
     "start_time": "2025-11-23T12:52:55.584041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "import time\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from IPython.core.display_functions import clear_output\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms"
   ],
   "id": "17a23fddca821883",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T12:52:58.790140Z",
     "start_time": "2025-11-23T12:52:58.785937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ACCEL_POS = (200, 850, 240, 850, 500)  #(x1, y1, x2, y2)\n",
    "BRAKE_POS = (2115, 850, 2200, 850, 500)  #(x1, y1, x2, y2)\n",
    "VIDEO_DEVICE = '/dev/video10'\n",
    "HOLD_DURATION_MS = 10_000  # 10 seconds (long enough to cover many steps)\n",
    "TAP_DURATION_MS = 1  # 10 seconds (long enough to cover many steps)"
   ],
   "id": "48b4d5cdb660d61b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T12:52:58.853017Z",
     "start_time": "2025-11-23T12:52:58.841559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_frame(cap):\n",
    "    \"\"\"Grab single frame from video device\"\"\"\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        raise RuntimeError(\"Failed to read frame\")\n",
    "    return frame\n",
    "\n",
    "\n",
    "def resize_img(frame, width=84):\n",
    "    \"\"\"Resize frame to given width, preserving aspect ratio (height auto-scaled)\"\"\"\n",
    "    h, w = frame.shape[:2]\n",
    "    new_w = width\n",
    "    new_h = int(h * (new_w / w))\n",
    "    resized = cv2.resize(frame, (new_w, new_h))\n",
    "    return resized\n",
    "\n",
    "\n",
    "def get_distance_img(frame):\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    # Reference resolution: (328, 720) ‚Üí h_ref=328, w_ref=720\n",
    "    h_ref, w_ref = 328, 720\n",
    "\n",
    "    # Scale coordinates proportionally\n",
    "    x1 = int(240 * w / w_ref)\n",
    "    y1 = int(24 * h / h_ref)\n",
    "    x2 = int((240 + 70) * w / w_ref)\n",
    "    y2 = int((24 + 19) * h / h_ref)\n",
    "\n",
    "    x1 = max(0, min(x1, w))\n",
    "    x2 = max(0, min(x2, w))\n",
    "    y1 = max(0, min(y1, h))\n",
    "    y2 = max(0, min(y2, h))\n",
    "\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return 0\n",
    "\n",
    "    roi = frame[y1:y2, x1:x2]\n",
    "    if roi.size == 0:\n",
    "        return 0\n",
    "\n",
    "    # Preprocess for Tesseract: B&W, high contrast\n",
    "    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY)\n",
    "    thresh = cv2.bitwise_not(thresh)\n",
    "\n",
    "    # display(Image.fromarray(thresh))\n",
    "    return thresh\n",
    "\n",
    "\n",
    "def split_digits(roi):\n",
    "    \"\"\"\n",
    "    Split a distance ROI into individual digit images.\n",
    "    Handles thin digits (like '1') by using morphological closing.\n",
    "    \"\"\"\n",
    "    if len(roi.shape) == 3:\n",
    "        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = roi.copy()\n",
    "\n",
    "    # Ensure white digits on black background\n",
    "    _, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # Connect nearby pixels (helps thin '1')\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    digit_bboxes = []\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        if w >= 2 and h >= 8:  # allow very thin but tall (like '1')\n",
    "            digit_bboxes.append((x, y, w, h))\n",
    "\n",
    "    digit_bboxes = sorted(digit_bboxes, key=lambda b: b[0])\n",
    "    digits = []\n",
    "    for (x, y, w, h) in digit_bboxes:\n",
    "        digit = roi[y:y + h, x:x + w]\n",
    "        digits.append(digit)\n",
    "\n",
    "    return digits\n",
    "\n",
    "\n",
    "def scale_pil_image(img: Image.Image, scale: int) -> Image.Image:\n",
    "    \"\"\"Scale up a PIL image by integer factor using nearest neighbor.\"\"\"\n",
    "    return img.resize((img.width * scale, img.height * scale), Image.NEAREST)"
   ],
   "id": "6abfe6f9282c57d7",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T12:52:59.207316Z",
     "start_time": "2025-11-23T12:52:58.903409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DigitClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=11):  # CHANGED: 11 classes (0-9 + 'm')\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((4, 4))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 4 * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)  # Now 11 outputs\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "digit_model = DigitClassifier(num_classes=11).to(device)\n",
    "digit_model.load_state_dict(torch.load(\"assets/digit_model.pth\", map_location=device))\n",
    "digit_model.eval()\n",
    "\n",
    "digit_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "def recognize_digit(img):\n",
    "    \"\"\"\n",
    "    Recognize a full distance string (e.g., '123m') from a single ROI image.\n",
    "    Input: grayscale numpy array (H, W) of full distance region\n",
    "    Output: string like '123m'\n",
    "    \"\"\"\n",
    "    global digit_model, digit_transform\n",
    "    digits = split_digits(img)\n",
    "    result = \"\"\n",
    "    for d in digits:\n",
    "        # Resize to 20x20 (must match training size)\n",
    "        d_resized = cv2.resize(d, (20, 20), interpolation=cv2.INTER_AREA)\n",
    "        d_pil = Image.fromarray(d_resized)\n",
    "        d_tensor = digit_transform(d_pil).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = digit_model(d_tensor)\n",
    "            pred = output.argmax().item()\n",
    "        char = 'm' if pred == 10 else str(pred)\n",
    "        result += char\n",
    "    return result"
   ],
   "id": "cc8633f1c3c1adc0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Get screen stream",
   "id": "455b1387fdb7a84b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T12:53:01.339856Z",
     "start_time": "2025-11-23T12:52:59.216524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Start scrcpy in background\n",
    "#scrcpy --v4l2-sink=/dev/video10 --no-window --stay-awake\n",
    "scrcpy_proc = subprocess.Popen([\n",
    "    'scrcpy', f'--v4l2-sink={VIDEO_DEVICE}',\n",
    "    '--no-window',\n",
    "    '--max-size', '720',\n",
    "    '--stay-awake'\n",
    "], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_DEVICE)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(f\"Cannot open {VIDEO_DEVICE}\")"
   ],
   "id": "f7797a6cc1980ac3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# test: view resized screen and log distance",
   "id": "a915501e07a5bf4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range(1000):\n",
    "    clear_output()\n",
    "\n",
    "    frame = get_frame(cap)\n",
    "    frame_img = Image.fromarray(frame)\n",
    "\n",
    "    h, w = frame.shape[:2]\n",
    "    print(f\"height: {h}  width: {w}\")\n",
    "\n",
    "    display(frame_img)\n",
    "\n",
    "    display(\n",
    "        Image.fromarray(get_distance_img(frame))\n",
    "    )\n",
    "\n",
    "    Image.fromarray(get_distance_img(frame)).save('abc.png')\n",
    "\n",
    "    print('OCR:', recognize_digit(get_distance_img(frame)))\n",
    "\n",
    "    print('-'*10)\n",
    "    time.sleep(0.1)"
   ],
   "id": "73cf05711d9a90db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# continue",
   "id": "e4e418406cf0c79f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fdfde8467eea16ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9b8f67fcd0df13e2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
